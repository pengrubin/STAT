sunflowerplot(data$Languange.A,data$VOT,col=0,seg.col=0,xlim = c(0.5,2.5),ylim = c(-90,20),xlab="Languange.A",ylab="VOT",xaxt='n',bty="n")
points(data$Languange.A,data$VOT,ylim=c(0,3))
axis(1, pos=0,at=0:3,labels=FALSE,lty = "solid")
title(main="Languange A")
text(x=1,y=-90,labels="Pre-voiced unaspirated")
text(x=2,y=-90,labels="Voiceless unspirated")
sunflowerplot(data$Languange.A,data$VOT,col=0,seg.col=0,xlim = c(0.5,2.5),ylim = c(-90,20),xlab="Languange.A",ylab="VOT",xaxt='n',bty="n")
points(data$Languange.A,data$VOT,ylim=c(0,3))
axis(1, pos=0,at=0:3,labels=FALSE,lty = "solid")
axis(1, pos=-80,at=0:3,labels=FALSE,lty = "solid")
title(main="Languange A")
text(x=1,y=-90,labels="Pre-voiced unaspirated")
text(x=2,y=-90,labels="Voiceless unspirated")
data <- read.csv("/Users/hongwei/Desktop/test2.csv")
View(data)
sunflowerplot(data$Languange.A,data$VOT,col=0,seg.col=0,xlim = c(0.5,2.5),ylim = c(-90,20),xlab="Languange.A",ylab="VOT",xaxt='n',bty="n")
sunflowerplot(data$Languange.A,data$VOT,col=0,seg.col=0,xlim = c(0.5,3.5),ylim = c(-90,20),xlab="Languange.A",ylab="VOT",xaxt='n',bty="n")
sunflowerplot(data$Languange.A,data$VOT,col=0,seg.col=0,xlim = c(0.5,3.5),xlab="Languange.A",ylab="VOT",xaxt='n',bty="n")
sunflowerplot(data$Languange.B,data$VOT,col=0,seg.col=0,xlim = c(0.5,3.5),xlab="Languange.A",ylab="VOT",xaxt='n',bty="n")
sunflowerplot(data$Language.B,data$VOT,col=0,seg.col=0,xlim = c(0.5,3.5),xlab="Languange.A",ylab="VOT",xaxt='n',bty="n")
points(data$Languange.A,data$VOT,ylim=c(0,3))
axis(1, pos=0,at=0:3,labels=FALSE,lty = "solid")
axis(1, pos=-80,at=0:3,labels=FALSE,lty = "solid")
title(main="Languange A")
text(x=1,y=-90,labels="Pre-voiced unaspirated")
points(data$Language.B,data$VOT,ylim=c(0,3))
axis(1, pos=0,at=0:3,labels=FALSE,lty = "solid")
axis(1, pos=-80,at=0:3,labels=FALSE,lty = "solid")
title(main="Languange A")
text(x=1,y=-90,labels="Pre-voiced unaspirated")
View(data)
data <- read.csv("/Users/hongwei/Desktop/test.csv")
View(data)
data <- read.csv("/Users/hongwei/Desktop/test2.csv")
sunflowerplot(data$Language.B,data$VOT,col=0,seg.col=0,xlim = c(0.5,3.5),xlab="Languange.A",ylab="VOT",xaxt='n',bty="n")
View(data)
sunflowerplot(data$Languange.A,data$VOT,col=0,seg.col=0,xlim = c(0.5,2.5),ylim = c(-90,20),xlab="Languange.A",ylab="VOT",xaxt='n',bty="n")
points(data$Languange.A,data$VOT,ylim=c(0,3))
data <- read.csv("/Users/hongwei/Desktop/test.csv")
sunflowerplot(data$Languange.A,data$VOT,col=0,seg.col=0,xlim = c(0.5,2.5),ylim = c(-90,20),xlab="Languange.A",ylab="VOT",xaxt='n',bty="n")
points(data$Languange.A,data$VOT,ylim=c(0,3))
sunflowerplot(data$Languange.A,data$VOT,col=0,seg.col=0,xlim = c(0.5,2.5),ylim = c(-90,20),xlab="Languange.A",ylab="VOT",xaxt='n',bty="n")
points(data$Languange.A,data$VOT)
points(data$Languange.A,data$VOT,ylim=c(0,3))
data <- read.csv("/Users/hongwei/Desktop/test2.csv")
sunflowerplot(data$Language.B,data$VOT,col=0,seg.col=0,xlim = c(0.5,3.5),xlab="Languange.A",ylab="VOT",xaxt='n',bty="n")
points(data$Language.B,data$VOT,ylim=c(0,3))
data <- read.csv("/Users/hongwei/Desktop/test2.csv")
sunflowerplot(data$Language.B,data$VOT,col=0,seg.col=0,xlim = c(0.5,3.5),xlab="Languange.B",ylab="VOT",xaxt='n',bty="n")
points(data$Language.B,data$VOT,ylim=c(0,3))
points(data$Language.B,data$VOT)
data <- read.csv("/Users/hongwei/Desktop/test2.csv")
sunflowerplot(data$Language.B,data$VOT,col=0,seg.col=0,xlim = c(0.5,3.5),xlab="Languange.B",ylab="VOT",xaxt='n',bty="n")
points(data$Language.B,data$VOT)
sunflowerplot(data$Language.B,data$VOT,col=0,seg.col=0,xlim = c(0.5,3.5),xlab="Languange.B",ylab="VOT",xaxt='n',bty="n")
sunflowerplot(data$Language.B,data$VOT)
data <- read.csv("/Users/hongwei/Desktop/test2.csv")
sunflowerplot(data$Language.B,data$VOT,col=0,seg.col=0,xlim = c(0.5,3.5),xlab="Languange.B",ylab="VOT",xaxt='n',bty="n")
points(data$Language.B,data$VOT)
axis(1, pos=0,at=0:3,labels=FALSE,lty = "solid")
axis(1, pos=-80,at=0:3,labels=FALSE,lty = "solid")
sunflowerplot(data$Language.B,data$VOT)
data <- read.csv("/Users/hongwei/Desktop/test2.csv")
sunflowerplot(data$Language.B,data$VOT,col=0,seg.col=0,xlim = c(0.5,3.5),xlab="Languange.B",ylab="VOT",xaxt='n',bty="n")
points(data$Language.B,data$VOT)
axis(1, pos=0,at=0:4,labels=FALSE,lty = "solid")
axis(1, pos=-80,at=0:4,labels=FALSE,lty = "solid")
title(main="Languange B")
text(x=1,y=-120,labels="Pre-voiced unaspirated")
text(x=2,y=-120,labels="Voiceless unspirated")
data <- read.csv("/Users/hongwei/Desktop/test2.csv")
sunflowerplot(data$Language.B,data$VOT,col=0,seg.col=0,xlim = c(0.5,3.5),ylim = c(-130,130),xlab="Languange.B",ylab="VOT",xaxt='n',bty="n")
points(data$Language.B,data$VOT)
axis(1, pos=0,at=0:4,labels=FALSE,lty = "solid")
axis(1, pos=-80,at=0:4,labels=FALSE,lty = "solid")
title(main="Languange B")
text(x=1,y=-120,labels="Pre-voiced unaspirated")
text(x=2,y=-120,labels="Voiceless unspirated")
data <- read.csv("/Users/hongwei/Desktop/test2.csv")
sunflowerplot(data$Language.B,data$VOT,col=0,seg.col=0,xlim = c(0.5,3.5),ylim = c(-130,130),xlab="Languange.B",ylab="VOT",xaxt='n',bty="n")
points(data$Language.B,data$VOT)
axis(1, pos=0,at=0:4,labels=FALSE,lty = "solid")
axis(1, pos=-80,at=0:4,labels=FALSE,lty = "solid")
title(main="Languange B")
text(x=1,y=-120,labels="Voiceless aspirated")
text(x=2,y=-120,labels="Pre-voiced unaspirated")
text(x=3,y=-120,labels="Voiceless unaspirated")
data <- read.csv("/Users/hongwei/Desktop/test2.csv")
sunflowerplot(data$Language.B,data$VOT,col=0,seg.col=0,xlim = c(0.5,3.5),ylim = c(-160,160),xlab="Languange.B",ylab="VOT",xaxt='n',bty="n")
points(data$Language.B,data$VOT)
axis(1, pos=0,at=0:4,labels=FALSE,lty = "solid")
axis(1, pos=-80,at=0:4,labels=FALSE,lty = "solid")
title(main="Languange B")
text(x=1,y=-120,labels="Voiceless aspirated")
text(x=2,y=-120,labels="Pre-voiced unaspirated")
text(x=3,y=-120,labels="Voiceless unaspirated")
data <- read.csv("/Users/hongwei/Desktop/test2.csv")
sunflowerplot(data$Language.B,data$VOT,col=0,seg.col=0,xlim = c(0.5,3.5),ylim = c(-150,160),xlab="Languange.B",ylab="VOT",xaxt='n',bty="n")
points(data$Language.B,data$VOT)
axis(1, pos=0,at=0:4,labels=FALSE,lty = "solid")
axis(1, pos=-80,at=0:4,labels=FALSE,lty = "solid")
title(main="Languange B")
text(x=1,y=-120,labels="Voiceless aspirated")
text(x=2,y=-120,labels="Pre-voiced unaspirated")
text(x=3,y=-120,labels="Voiceless unaspirated")
data <- read.csv("/Users/hongwei/Desktop/test2.csv")
sunflowerplot(data$Language.B,data$VOT,col=0,seg.col=0,xlim = c(0.5,3.5),ylim = c(-130,160),xlab="Languange.B",ylab="VOT",xaxt='n',bty="n")
points(data$Language.B,data$VOT)
axis(1, pos=0,at=0:4,labels=FALSE,lty = "solid")
axis(1, pos=-80,at=0:4,labels=FALSE,lty = "solid")
title(main="Languange B")
text(x=1,y=-120,labels="Voiceless aspirated")
text(x=2,y=-120,labels="Pre-voiced unaspirated")
text(x=3,y=-120,labels="Voiceless unaspirated")
data <- read.csv("/Users/hongwei/Desktop/test2.csv")
sunflowerplot(data$Language.B,data$VOT,col=0,seg.col=0,xlim = c(0.5,3.5),ylim = c(-130,160),xlab="Languange.B",ylab="VOT",xaxt='n',bty="n")
points(data$Language.B,data$VOT)
axis(1, pos=0,at=0:4,labels=FALSE,lty = "solid")
axis(1, pos=-110,at=0:4,labels=FALSE,lty = "solid")
title(main="Languange B")
text(x=1,y=-120,labels="Voiceless aspirated")
text(x=2,y=-120,labels="Pre-voiced unaspirated")
text(x=3,y=-120,labels="Voiceless unaspirated")
data <- read.csv("/Users/hongwei/Desktop/test2.csv")
sunflowerplot(data$Language.B,data$VOT,col=0,seg.col=0,xlim = c(0.5,3.5),ylim = c(-130,160),xlab="Languange.B",ylab="VOT",xaxt='n',bty="n")
points(data$Language.B,data$VOT)
axis(1, pos=0,at=0:4,labels=FALSE,lty = "solid")
axis(1, pos=-110,at=0:4,labels=FALSE,lty = "solid")
title(main="Languange B")
text(x=1,y=-120,labels="Pre-voiced unaspirated")
text(x=2,y=-120,labels="Voiceless aspirated")
text(x=3,y=-120,labels="Voiceless unaspirated")
library(IRTShiny)
?IRTShiny
startIRT()
startIRT()
data <- read.csv("/Users/hongwei/Desktop/IRTtest1.csv")
View(data)
startIRT()
## S3 method for class 'tpm'
plot(x, type = c("ICC", "IIC"), items = NULL,
zrange = c(-3.8, 3.8), z = seq(zrange[1], zrange[2], length = 100),
annot, labels = NULL, legend = FALSE, cx = "topleft", cy = NULL,
ncol = 1, bty = "n", col = palette(), lty = 1, pch, xlab, ylab, main,
sub = NULL, cex = par("cex"), cex.lab = par("cex.lab"),
cex.main = par("cex.main"), cex.sub = par("cex.sub"),
cex.axis = par("cex.axis"), plot = TRUE, ...)
fit <- grm(Science[c(1,3,4,7)])
## Item Response Category Characteristic Curves for
## the Science data
op <- par(mfrow = c(2, 2))
plot(fit, lwd = 2, legend = TRUE, ncol = 2)
par(op)
## Item Characteristic Curves for the 2nd category,
## and items 1 and 3
plot(fit, category = 2, items = c(1, 3), lwd = 2, legend = TRUE, cx = "right")
## Item Information Curves for the Science data;
plot(fit, type = "IIC", legend = TRUE, cx = "topright", lwd = 2, cex = 1.4)
View(fit)
plot(fit, lwd = 2, legend = TRUE, ncol = 2)
plot(fit, category = 2, items = c(1, 3), lwd = 2, legend = TRUE, cx = "right")
## Item Information Curves for the Science data;
plot(fit, type = "IIC", legend = TRUE, cx = "topright", lwd = 2, cex = 1.4)
## Test Information Function for the Science data;
plot(fit, type = "IIC", items = 0, lwd = 2)
fit
Science[c(1, 3, 4, 7)]
beta <- seq(3,3.6,0.01)
logl.beta <- vector("numeric",length(beta))
#
# Evaluate the log-likelihood, score and information functions
# for each value of beta. We can’t easily avoid a loop for the
# log-likelihood, but the other 2 functions can each be
# evaluated in a single vector operation (which is more efficient)
#
for (i in 1:length(beta)) {
logl.beta[i] <- sum(log(dpois(storm.data$Storms,exp(beta[i]))))
}
n <- length(storm.data$Storms)
beta <- seq(3,3.6,0.01)
logl.beta <- vector("numeric",length(beta))
#
# Evaluate the log-likelihood, score and information functions
# for each value of beta. We can’t easily avoid a loop for the
# log-likelihood, but the other 2 functions can each be
# evaluated in a single vector operation (which is more efficient)
#
for (i in 1:length(beta)) {
logl.beta[i] <- sum(log(dpois(storm.data$Storms,exp(beta[i]))))
}
#
# Evaluate the log-likelihood, score and information functions
# for each value of beta. We can’t easily avoid a loop for the
# log-likelihood, but the other 2 functions can each be
# evaluated in a single vector operation (which is more efficient)
#
for (i in 1:length(beta)) {
logl.beta[i] <- sum(log(dpois(storm.data$Storms,exp(beta[i]))))
}
storage.mode()
beta <- seq(3,3.6,0.01)
logl.beta <- vector("numeric",length(beta))
#
# Evaluate the log-likelihood, score and information functions
# for each value of beta. We can’t easily avoid a loop for the
# log-likelihood, but the other 2 functions can each be
# evaluated in a single vector operation (which is more efficient)
#
for (i in 1:length(beta)) {
logl.beta[i] <- sum(log(dpois(storm.data$Storms,exp(beta[i]))))
}
#
# This is an R function to illustrate the use of Iterative Weighted
# Least Squares for estimating the log of a Poisson mean. The arguments
# are y (a vector of counts, assumed IID Poisson) and startval, an
# initial guess at the log mean. The `Steps' refer to Section 3 of the
# Lab 8 lecture notes.
#
IWLS <- function(y,startval) {
n <- length(y)                  #          For dimensioning
X <- as.matrix(rep(1,n))        # Step 1:  assemble the matrix X
betahat <- startval             # Step 2:  initial value
U <- 10                         #          Define a value for the
#          score, U (this is just
#          so that the test for
#          convergence on the next
#          line doesn't fail on the
#          first attempt because we
#          haven't defined U yet)
iter <- 0                       #          Initialise iteration count
while(abs(U) > 1e-6) {
eta <- as.vector(X%*%betahat) # Step 3:  calculate linear
mu <- exp(eta)                #          predictors, means
V <- mu                       #          and variances
W <- (mu^2)/V                 # Step 4:  diagonal elements of W
z <- eta + ( (y-mu)/mu )      # Step 5:  adjusted dependent variate
XW <- t(W*X)                  # Step 6:  calculation of X'W
#          (uses elementwise
#          multiplication, exploiting
#          the fact that W will be
#          recycled to match the number
#          of elements in X)
XWX <- solve(XW%*%X)          # Step 7:  calculation of [X'WX]^-1,
XWz <- XW%*%z                 #          X'Wz and U
U <- XW%*%(z-eta)
cat(paste("Iteration",iter,   #          Output current values to
" Estimate",        #          screen (rounded to a
round(betahat,6),   #          sensible number of decimal
" Score",           #          places)
round(U,8),"\n"))   #
betahat <- XWX%*%XWz          # Step 8:  update betahat, and go back
iter <- iter + 1              #          if necessary
}
phi <- 1                        # Step 9:  not strictly necessary here
beta.se <- sqrt(diag(XWX))      # Step 10: calculate standard errors
mle.table <-
data.frame(Estimate=betahat,  # Step 11: assemble results into a
S.E.=beta.se,      #          data frame, and return
T=betahat/beta.se) #
mle.table
}
beta <- seq(3,3.6,0.01)
logl.beta <- vector("numeric",length(beta))
#
# Evaluate the log-likelihood, score and information functions
# for each value of beta. We can’t easily avoid a loop for the
# log-likelihood, but the other 2 functions can each be
# evaluated in a single vector operation (which is more efficient)
#
for (i in 1:length(beta)) {
logl.beta[i] <- sum(log(dpois(storm.data$Storms,exp(beta[i]))))
}
n <- length(storm.data$Storms)
storm.data <- read.csv("/Users/hongwei/Desktop/nstorms.dat.txt")
#
# Evaluate the log-likelihood, score and information functions
# for each value of beta. We can’t easily avoid a loop for the
# log-likelihood, but the other 2 functions can each be
# evaluated in a single vector operation (which is more efficient)
#
for (i in 1:length(beta)) {
logl.beta[i] <- sum(log(dpois(storm.data$Storms,exp(beta[i]))))
}
View(storm.data)
storm.data <- read.csv("/Users/hongwei/Desktop/nstorms.dat.txt",sep=" ")
#
# Evaluate the log-likelihood, score and information functions
# for each value of beta. We can’t easily avoid a loop for the
# log-likelihood, but the other 2 functions can each be
# evaluated in a single vector operation (which is more efficient)
#
for (i in 1:length(beta)) {
logl.beta[i] <- sum(log(dpois(storm.data$Storms,exp(beta[i]))))
}
n <- length(storm.data$Storms)
ysum <- sum(storm.data$Storms)
u.beta <- DEFINE THE SCORE FUNCTION HERE (FORMULA IS GIVEN ABOVE)
i.beta <- DEFINE THE INFORMATION FUNCTION HERE (       "       )
#
# Set up a graphics screen with space for 3 plots, and plot.
source("/Users/hongwei/Documents/GitHub/STAT/pjnfunctions.R")
source("/Users/hongwei/Documents/GitHub/STAT/pjnfunctions.R")
source("/Users/hongwei/Documents/GitHub/STAT/STAT0017_Practical_2/pjnfunctions.R")
source("/Users/hongwei/Documents/GitHub/STAT/STAT0017_Practical_2/pjnfunctions.R")
install.packages("devtools")
library("devtools")
install_github("ConstantianosChr/exdex")
library("devtools")
install_github("ConstantianosChr/exdex")
install_github("ConstantinosChr/exdex")
View(gev.cdf)
library(ismev)
data(dowjones)
install.packages("ismev")
install.packages("dowjones")
library(ismev)
data(dowjones)
?dowjones  # in ismev
DJ <- dowjones[,2]                                  # Dow Jones index
t <- (dowjones[,1]-dowjones[1,1])/(3600*24*365)     # time in years
View(dowjones)
t
plot(t+1996,DJ,xlab="years",ylab="Dow Jones index") # TS plot
DJ.ret <- -100*diff(log(DJ)) # negated % daily log-returns (so large values
# correspond to big falls in the index)
t <- t[-1]
plot(t+1996,DJ.ret,xlab="years",ylab="% daily losses") # variability increases over time?
library(evd)
install.packages("evd")
library(evd)
par(mfrow=c(1,1))
evd::mrlplot(DJ.ret)
tlim <- c(-1,3)
evd::mrlplot(DJ.ret,tlim)
tlim <- quantile(DJ.ret,probs=c(0.1,0.99))  # try changing the probs argument
# [tcplot in evd is an alternative to gpd.fitrange in ismev.]
tcplot(DJ.ret,tlim)
u <- 0.5                # pick a threshold
p.u <- mean(DJ.ret > u) # prob of exceedance
fit <- fpot(DJ.ret,threshold=u)  # evd alternatives to ismev's gpd.fit and gpd.diag
fit
par(mfrow=c(2,2))
plot(fit)
x(11)
x()
x11
library(ismev)
data(dowjones)
?dowjones  # in ismev
DJ <- dowjones[,2]                                  # Dow Jones index
t <- (dowjones[,1]-dowjones[1,1])/(3600*24*365)     # time in years
plot(t+1996,DJ,xlab="years",ylab="Dow Jones index") # TS plot
DJ.ret <- -100*diff(log(DJ)) # negated % daily log-returns (so large values
# correspond to big falls in the index)
t <- t[-1]
plot(t+1996,DJ.ret,xlab="years",ylab="% daily losses") # variability increases over time?
plot(t+1996,DJ.ret,xlab="years",ylab="% daily losses") # variability increases over time?
plot(t+1996,DJ.ret,xlab="years",ylab="% daily losses") # variability increases over time?
library(evd)
par(mfrow=c(1,1))
evd::mrlplot(DJ.ret)
tlim <- c(-1,3)
evd::mrlplot(DJ.ret,tlim)
tlim <- quantile(DJ.ret,probs=c(0.1,0.99))  # try changing the probs argument
# [tcplot in evd is an alternative to gpd.fitrange in ismev.]
tcplot(DJ.ret,tlim)
u <- 0.5                # pick a threshold
p.u <- mean(DJ.ret > u) # prob of exceedance
fit <- fpot(DJ.ret,threshold=u)  # evd alternatives to ismev's gpd.fit and gpd.diag
fit
par(mfrow=c(2,2))
plot(fit)
?plot.uvevd  # in evd
par(mfrow=c(2,2))
plot(fit)
?plot.uvevd  # in evd
par(mfrow=c(2,1)) # Profile log-likelihoods
plot(profile(fit))
plot(profile(fit))
fit.ismev <- gpd.fit(DJ.ret,threshold=u)
gpd.diag(fit.ismev)
gpd.diag(fit.ismev)
fit.ismev <- gpd.fit(DJ.ret,threshold=u)
gpd.diag(fit.ismev)
fit.ismev <- gpd.fit(DJ.ret,threshold=u)
gpd.diag(fit.ismev)
ydat <- matrix(t,ncol=1)
# sigma.u linear in time
fit.year <- gpd.fit(DJ.ret,threshold=u,ydat=ydat,sigl=1)
# ln(sigma.u) linear in time
fit.year <- gpd.fit(DJ.ret,threshold=u,ydat=ydat,sigl=1,siglink=exp)
gpd.diag(fit.year)                                       # constant threshold u
# Likelihood ratio test
pchisq(2*(fit.ismev$nllh-fit.year$nllh),1,lower.tail=F) # p-value
library(quantreg)
rq.thresh <- rq(DJ.ret ~ t, tau = 1 - p.u) # quantile regression
u.t <- fitted(rq.thresh)            # threshold
plot(t+1996,DJ.ret,xlab="years",ylab="% daily losses") # variability increases over time?
lines(t+1996,u.t,col="red",lwd=2)
fit.pp <- pjn.pp.fit(DJ.ret,threshold=u.t)
np1 <- length(fit.pp$mle)
fit.mu <- pjn.pp.fit(DJ.ret,threshold=u.t,ydat=ydat,mul=1)
np2 <- length(fit.mu$mle)
fit.sig <- pjn.pp.fit(DJ.ret,threshold=u.t,ydat=ydat,sigl=1)
np3 <- length(fit.sig$mle)
fit.musig <- pjn.pp.fit(DJ.ret,threshold=u.t,ydat=ydat,mul=1,sigl=1)
np4 <- length(fit.musig$mle)
nllhs <- c(fit.pp$nllh,fit.mu$nllh,fit.sig$nllh,fit.musig$nllh)
npars <- c(np1,np2,np3,np4)
cbind(nllhs,npars)
pp.diag(fit.mu) # diagnostic plots
library(ismev)
data(fremantle) # See page 111 of Coles (2001)
?fremantle # in ismev
plot(fremantle[,1],fremantle[,2],pch=16,xlab="year",ylab="sea level (m)")
plot(fremantle[,3],fremantle[,2],pch=16,xlab="SOI",ylab="sea level (m)")
?fremantle # in ismev
plot(fremantle[,1],fremantle[,2],pch=16,xlab="year",ylab="sea level (m)")
plot(fremantle[,3],fremantle[,2],pch=16,xlab="SOI",ylab="sea level (m)")
fit0 <- gev.fit(fremantle[,2])                  # no covariates
gev.diag(fit0)                                  # model diagnostics
ydat <- fremantle[,c(1,3)]                      # year in column 1, SOI in column 2
Y1 = as.matrix( read.table(file="series1.dat") )
# Loading the data
setwd("/Users/hongwei/Documents/GitHub/STAT/STAT0010_Workshop/")
Y1 = as.matrix( read.table(file="series1.dat") )
Y2 = as.matrix( read.table(file="series2.dat") )
Y3 = as.matrix( read.table(file="series3.dat") )
Y4 = as.matrix( read.table(file="series4.dat") )
ts.plot(Y1)
par(mfrow=c(2,1))
acf(Y1); pacf(Y1)
acf(Y1); pacf(Y1)
diff.Y1 = diff(Y1)
par(mfrow=c(1,1))
ts.plot(diff.Y1)
par(mfrow=c(2,1))
acf(diff.Y1); pacf(diff.Y1)
par(mfrow=c(1,1))
par(mfrow=c(1,1))
ts.plot(Y2)
par(mfrow=c(2,1))
acf(Y2); pacf(Y2)
ts.plot(Y3)
par(mfrow=c(2,1))
acf(Y3); pacf(Y3)
diff.Y3 = diff(Y3)
ts.plot(diff.Y3)
par(mfrow=c(2,1))
acf(diff.Y3); pacf(diff.Y3)
ts.plot(Y4)
par(mfrow=c(2,1))
acf(Y4); pacf(Y4)
data = as.matrix( read.table(file="lottery.dat") )
Y = data[,4]
ts.plot(Y)
diff.Y = diff(Y)
ts.plot(diff.Y)
par(mfrow=c(2,1))
acf(diff.Y); pacf(diff.Y)
plot(diff.Y[-length(diff.Y)], diff.Y[-1])
data = as.matrix( read.table(file="birr.dat") )
Y = data[,3]
ts.plot(Y)
data[Y>150,]  # show the data where Y>150
ts.plot(Y)
par(mfrow=c(2,1))
acf(Y); pacf(Y)
diff.Y = diff(Y,lag=12)
ts.plot(diff.Y)
par(mfrow=c(2,1))
acf(diff.Y); pacf(diff.Y)
fit1 = arima(Y, order=c(0,0,0), seasonal=list(order=c(1,0,0),period=12))
ts.plot(fit1$residuals)
par(mfrow=c(2,1))
acf(fit1$residuals); pacf(fit1$residuals)
fit2 = arima(Y, order=c(0,0,0), seasonal=list(order=c(0,1,1),period=12))
ts.plot(fit2$residuals)
par(mfrow=c(2,1))
acf(fit2$residuals); pacf(fit2$residuals)
fit2$coef
fit2 = arima(Y, order=c(0,0,0), seasonal=list(order=c(0,1,1),period=12))
ts.plot(fit2$residuals)
par(mfrow=c(2,1))
diff.Y1 = diff(Y1)
par(mfrow=c(1,1))
ts.plot(diff.Y1)
par(mfrow=c(2,1))
acf(diff.Y1); pacf(diff.Y1)
par(mfrow=c(1,1))
ts.plot(Y2)
par(mfrow=c(2,1))
acf(Y2); pacf(Y2)
ts.plot(Y3)
par(mfrow=c(2,1))
acf(Y3); pacf(Y3)
diff.Y3 = diff(Y3)
ts.plot(diff.Y3)
par(mfrow=c(2,1))
acf(diff.Y3); pacf(diff.Y3)
ts.plot(Y4)
par(mfrow=c(2,1))
acf(Y4); pacf(Y4)
data = as.matrix( read.table(file="lottery.dat") )
Y = data[,4]
ts.plot(Y)
diff.Y = diff(Y)
ts.plot(diff.Y)
par(mfrow=c(2,1))
ts.plot(Y)
diff.Y = diff(Y)
ts.plot(diff.Y)
